{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20220515_ensamble.ipynb","provenance":[],"authorship_tag":"ABX9TyNJ43UjfaStme3oDkWIArkn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Python: アンサンブル学習の Voting を試す\n","https://blog.amedama.jp/entry/2018/12/16/134028\n","\n","今回は機械学習におけるアンサンブル学習の一種として Voting という手法を試してみる。 これは、複数の学習済みモデルを用意して多数決などで推論の結果を決めるという手法。 この手法を用いることで最終的なモデルの性能を上げられる可能性がある。 実装については自分で書いても良いけど scikit-learn に使いやすいものがあったので、それを選んだ。"],"metadata":{"id":"U-YXDGg-BrLh"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","以下のサンプルコードでは乳がんデータセットを使って Voting を試している。 使ったモデルはサポートベクターマシン、ランダムフォレスト、ロジスティック回帰、k-最近傍法、ナイーブベイズの五つ。 モデルの性能は 5-Fold CV を使って精度 (Accuracy) について評価している。"],"metadata":{"id":"nnkqyGnBB4ht"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkPypriXBL2s","executionInfo":{"status":"ok","timestamp":1652623474714,"user_tz":-540,"elapsed":19917,"user":{"displayName":"宮内裕史","userId":"17299514912379799424"}},"outputId":"16e868b4-04e1-43ae-9813-2689c0bfd6fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:17<00:00,  3.57s/it]"]},{"output_type":"stream","name":"stdout","text":["voting : 0.9543238627542306\n","svm : 0.9138953578636858\n","rf : 0.9578326346840551\n","logit : 0.9543393882937432\n","knn : 0.9350100916006833\n","nb : 0.9385343890700202\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from collections import defaultdict\n","\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn import datasets\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.naive_bayes import GaussianNB\n","\n","def main():\n","    # 乳がんデータセットを読み込む\n","    dataset = datasets.load_breast_cancer()\n","    X, y = dataset.data, dataset.target\n","\n","    # voting に使う分類器を用意する\n","    estimators = [\n","        ('svm', SVC(gamma='scale', probability=True)),\n","        ('rf', RandomForestClassifier(n_estimators=100)),\n","        ('logit', LogisticRegression(solver='lbfgs', max_iter=10000)),\n","        ('knn', KNeighborsClassifier()),\n","        ('nb', GaussianNB()),\n","    ]\n","\n","    accs = defaultdict(list)\n","\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    for train_index, test_index in tqdm(list(skf.split(X, y))):\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        # 分類器を学習する\n","        voting = VotingClassifier(estimators)\n","        voting.fit(X_train, y_train)\n","\n","        # アンサンブルで推論する\n","        y_pred = voting.predict(X_test)\n","        acc = accuracy_score(y_test, y_pred)\n","        accs['voting'].append(acc)\n","\n","        # 個別の分類器の性能も確認してみる\n","        for name, estimator in voting.named_estimators_.items():\n","            y_pred = estimator.predict(X_test)\n","            acc = accuracy_score(y_test, y_pred)\n","            accs[name].append(acc)\n","\n","    for name, acc_list in accs.items():\n","        mean_acc = np.array(acc_list).mean()\n","        print(name, ':', mean_acc)\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"markdown","source":["なんと Voting するよりもランダムフォレスト単体の方が性能が良いという結果になってしまった。 このように Voting するからといって必ずしも性能が上がるとは限らない。 例えば今回のように性能が突出したモデルがあるなら、それ単体で使った方が良くなる可能性はある。 あるいは、極端に性能が劣るモデルがあるならそれは取り除いた方が良いかもしれない。 それ以外には、次の項目で説明するモデルの重み付けという手もありそう。"],"metadata":{"id":"2ijvBpM6ChrA"}},{"cell_type":"markdown","source":["モデルに重みをつける\n","\n","\n","---\n","\n","\n","性能が突出したモデルを単体で使ったり、あるいは劣るモデルを取り除く以外の選択肢として、モデルの重み付けがある。 これは、多数決などで推論結果を出す際に、特定のモデルの意見を重要視・あるいは軽視するというもの。 scikit-learn の VotingClassifier であれば weights というオプションでモデルの重みを指定できる。\n","\n","次のサンプルコードでは、ランダムフォレストとロジスティック回帰の意見を重要視するように重みをつけてみた。"],"metadata":{"id":"9IgSmP84CrjE"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn import datasets\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.naive_bayes import GaussianNB\n","\n","\n","def main():\n","    dataset = datasets.load_breast_cancer()\n","    X, y = dataset.data, dataset.target\n","\n","    estimators = [\n","        ('svm', SVC(gamma='scale', probability=True)),\n","        ('rf', RandomForestClassifier(n_estimators=100)),\n","        ('logit', LogisticRegression(solver='lbfgs', max_iter=10000)),\n","        ('knn', KNeighborsClassifier()),\n","        ('nb', GaussianNB()),\n","    ]\n","\n","    accs = defaultdict(list)\n","\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    for train_index, test_index in tqdm(list(skf.split(X, y))):\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        # 分類器に重みをつける\n","        voting = VotingClassifier(estimators,\n","                                  weights=[1, 2, 2, 1, 1])\n","        voting.fit(X_train, y_train)\n","\n","        y_pred = voting.predict(X_test)\n","        acc = accuracy_score(y_test, y_pred)\n","        accs['voting'].append(acc)\n","\n","        for name, estimator in voting.named_estimators_.items():\n","            y_pred = estimator.predict(X_test)\n","            acc = accuracy_score(y_test, y_pred)\n","            accs[name].append(acc)\n","\n","    for name, acc_list in accs.items():\n","        mean_acc = np.array(acc_list).mean()\n","        print(name, ':', mean_acc)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtxZTFIECyFr","executionInfo":{"status":"ok","timestamp":1652623624351,"user_tz":-540,"elapsed":8040,"user":{"displayName":"宮内裕史","userId":"17299514912379799424"}},"outputId":"34b9c121-9a68-46f0-bbe3-5b082550cee9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:07<00:00,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["voting : 0.9560937742586555\n","svm : 0.9138953578636858\n","rf : 0.9560782487191428\n","logit : 0.9543393882937432\n","knn : 0.9350100916006833\n","nb : 0.9385343890700202\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["Seed Averaging\n","\n","\n","---\n","\n","\n","先ほどの例では、モデルに重み付けしてみたものの結局ランダムフォレストを単体で使った方が性能が良かった。 とはいえ Voting は一つのアルゴリズムだけを使う場合にも性能向上につなげる応用がある。 それが、続いて紹介する Seed Averaging という手法。 これは、同じアルゴリズムでも学習に用いるシード値を異なるものにしたモデルを複数用意して Voting するというやり方。\n","\n","次のサンプルコードでは、Voting で使うアルゴリズムはランダムフォレストだけになっている。 ただし、初期化するときのシード値がそれぞれ異なっている。"],"metadata":{"id":"Ygj-kMRoDNGo"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn import datasets\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","def main():\n","    dataset = datasets.load_breast_cancer()\n","    X, y = dataset.data, dataset.target\n","\n","    # Seed Averaging\n","    estimators = [\n","        ('rf1', RandomForestClassifier(n_estimators=100, random_state=0)),\n","        ('rf2', RandomForestClassifier(n_estimators=100, random_state=1)),\n","        ('rf3', RandomForestClassifier(n_estimators=100, random_state=2)),\n","        ('rf4', RandomForestClassifier(n_estimators=100, random_state=3)),\n","        ('rf5', RandomForestClassifier(n_estimators=100, random_state=4)),\n","    ]\n","\n","    accs = defaultdict(list)\n","\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    for train_index, test_index in tqdm(list(skf.split(X, y))):\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        voting = VotingClassifier(estimators)\n","        voting.fit(X_train, y_train)\n","\n","        y_pred = voting.predict(X_test)\n","        acc = accuracy_score(y_test, y_pred)\n","        accs['voting'].append(acc)\n","\n","        for name, estimator in voting.named_estimators_.items():\n","            y_pred = estimator.predict(X_test)\n","            acc = accuracy_score(y_test, y_pred)\n","            accs[name].append(acc)\n","\n","    for name, acc_list in accs.items():\n","        mean_acc = np.array(acc_list).mean()\n","        print(name, ':', mean_acc)\n","\n","\n","if __name__ == '__main__':\n","    main()  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6CF8LezDXtp","executionInfo":{"status":"ok","timestamp":1652623777885,"user_tz":-540,"elapsed":7682,"user":{"displayName":"宮内裕史","userId":"17299514912379799424"}},"outputId":"3e54768c-2d30-4e6f-95e8-43862c1c2be4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:07<00:00,  1.46s/it]"]},{"output_type":"stream","name":"stdout","text":["voting : 0.956078248719143\n","rf1 : 0.9543238627542306\n","rf2 : 0.9543083372147182\n","rf3 : 0.9560782487191428\n","rf4 : 0.9578326346840551\n","rf5 : 0.9578326346840551\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["今回は、最も性能の良い三番目のモデルよりも、わずかながら Voting した結果の方が性能が良くなっている。 これは、各モデルの推論結果を平均することで、最終的なモデルの識別境界がなめらかになる作用が期待できるためと考えられる。"],"metadata":{"id":"jvmvgTPRFTet"}},{"cell_type":"markdown","source":["Soft Voting と Hard Voting\n","\n","\n","---\n","\n","\n","Voting と一口に言っても、推論結果の出し方には Soft Voting と Hard Voting という二つのやり方がある。 分かりやすいのは Hard Voting で、これは単純に各モデルの意見を多数決で決めるというもの。 もうひとつの Soft Voting は、それぞれのモデルの出した推論結果の確率を平均するというもの。 そこで、続いては、それぞれの手法について詳しく見ていくことにする。"],"metadata":{"id":"-ygcFnhmFYbd"}}]}